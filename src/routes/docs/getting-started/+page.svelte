<script lang="ts">
	import DocsLayout from '$lib/components/docs/DocsLayout.svelte';
</script>

<svelte:head>
	<title>Getting Started - VCP Documentation</title>
	<meta name="description" content="Quick start guide for implementing VCP in your application." />
	<link rel="canonical" href="https://valuecontextprotocol.org/docs/getting-started/" />
	<meta property="og:title" content="Getting Started - VCP Documentation" />
	<meta property="og:description" content="Quick start guide for implementing VCP in your application." />
	<meta property="og:url" content="https://valuecontextprotocol.org/docs/getting-started/" />
	<meta property="og:image" content="https://valuecontextprotocol.org/vcp-logo.png" />
	<meta name="twitter:card" content="summary" />
</svelte:head>

<DocsLayout
	title="Getting Started"
	description="Get up and running with VCP in 5 minutes."
>
	{#snippet children()}
		<h2>What is VCP?</h2>
		<p>
			The <strong>Value Context Protocol (VCP)</strong> is a standard for encoding user preferences,
			constraints, and context as portable categorical flags. Set it once, change it in real time,
			and every connected service adapts instantly. Private details stay on-device by design.
		</p>

		<p>
			VCP is built on three pillars:
		</p>
		<ul>
			<li><strong>Portability</strong> â€”Your context travels with you across platforms and services</li>
			<li><strong>Adaptation</strong> â€”AI behavior shifts automatically as your situation changes</li>
			<li><strong>Liveness</strong> â€”Personal state updates in real time, shaping responses moment to moment</li>
		</ul>

		<blockquote>
			"Context that travels with you, wherever you need it."
		</blockquote>

		<h2>Quick Start</h2>

		<h3>1. Define a VCP Context</h3>
		<p>A VCP context contains your preferences, constraints, and personal state:</p>

		<pre><code>{`import type { VCPContext } from 'vcp';

const context: VCPContext = {
  vcp_version: "1.0",
  profile_id: "user_001",

  // Reference a constitution (behavioral guidelines)
  constitution: {
    id: "learning-assistant",
    version: "1.0",
    persona: "muse",
    adherence: 3
  },

  // Public preferences - shared with all stakeholders
  public_profile: {
    goal: "learn_guitar",
    experience: "beginner",
    learning_style: "visual"
  },

  // Portable preferences - follow you across platforms
  portable_preferences: {
    noise_mode: "quiet_preferred",
    session_length: "30_minutes",
    budget_range: "low"
  },

  // Private context - influences AI but NEVER exposed
  private_context: {
    _note: "Values here shape recommendations but are never transmitted",
    work_situation: "unemployed",
    housing_situation: "living_with_parents"
  },

  // Personal state (v3.1) - real-time user state
  personal_state: {
    cognitive_state: { value: "focused", intensity: 3 },
    emotional_tone: { value: "calm", intensity: 4 },
    energy_level: { value: "rested", intensity: 3 },
    perceived_urgency: { value: "unhurried", intensity: 2 },
    body_signals: { value: "neutral", intensity: 1 }
  }
};`}</code></pre>

		<h3>2. Encode to CSM-1 Token</h3>
		<p>The <strong>CSM-1 (Compact State Message)</strong> format is a human-readable token that encodes your context:</p>

		<pre><code>{`import { encodeContextToCSM1 } from 'vcp';

const token = encodeContextToCSM1(context);

// Output:
// VCP:1.0:user_001
// C:learning-assistant@1.0
// P:muse:3
// G:learn_guitar:beginner:visual
// X:ğŸ”‡quiet:ğŸ’°low:â±ï¸30minutes
// F:none
// S:ğŸ”’work|ğŸ”’housing
// R:ğŸ§ focused:3|ğŸ’­calm:4|ğŸ”‹rested:3|âš¡unhurried:2|ğŸ©ºneutral:1`}</code></pre>

		<h3>3. Share with Stakeholders</h3>
		<p>The token carries what each service needs to adapt â€” compact, categorical, and instantly parseable:</p>

		<table>
			<thead>
				<tr>
					<th>Line</th>
					<th>Meaning</th>
					<th>AI Sees</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td><code>G:learn_guitar:beginner:visual</code></td>
					<td>Goal + skill level + style</td>
					<td>âœ“ Full detail</td>
				</tr>
				<tr>
					<td><code>X:ğŸ”‡quiet:ğŸ’°low</code></td>
					<td>Noise + budget constraints</td>
					<td>âœ“ Flags only</td>
				</tr>
				<tr>
					<td><code>S:ğŸ”’work|ğŸ”’housing</code></td>
					<td>Private context exists</td>
					<td>âœ— Categories only</td>
				</tr>
				<tr>
					<td><code>R:ğŸ§ focused:3|ğŸ’­calm:4|...</code></td>
					<td>Real-time personal state</td>
					<td>âœ“ Shapes response style</td>
				</tr>
			</tbody>
		</table>

		<p>
			The AI knows <em>that</em> work and housing context influenced the recommendations, but not
			<em>what</em> that context is. This enables personalization without surveillance.
		</p>

		<h2>Key Concepts</h2>

		<h3>Privacy Levels</h3>
		<ul>
			<li><strong>Public</strong> â€” Always shared (goals, experience level)</li>
			<li><strong>Consent</strong> â€” Shared when you approve (specific preferences)</li>
			<li><strong>Private</strong> â€” Never transmitted, only influences locally (sensitive reasons)</li>
		</ul>

		<h3>Constitutions</h3>
		<p>
			Constitutions define AI behavioral guidelines. They specify what an AI should prioritize,
			avoid, and how it should interact. VCP contexts reference constitutions to ensure consistent
			behavior.
		</p>

		<h3>Personas</h3>
		<p>Different interaction styles built into constitutions:</p>
		<ul>
			<li><strong>Muse</strong> â€” Creative, exploratory, encouraging</li>
			<li><strong>Ambassador</strong> â€” Professional, diplomatic, balanced</li>
			<li><strong>Godparent</strong> â€” Nurturing, supportive, patient</li>
			<li><strong>Sentinel</strong> â€” Cautious, protective, conservative</li>
			<li><strong>Nanny</strong> â€” Structured, directive, safe</li>
			<li><strong>Mediator</strong> â€” Calm, structured, empathetic</li>
		</ul>

		<h2>See It in Action</h2>
		<p>Each pillar comes alive in a persona-driven demo:</p>
		<ul>
			<li><a href="/demos/gentian">Gentian</a> â€” <strong>Portability</strong>: Watch a single VCP token travel across three guitar-learning platforms</li>
			<li><a href="/demos/campion">Campion</a> â€” <strong>Adaptation</strong>: See how context-switching between work and home personas changes AI behavior instantly</li>
			<li><a href="/demos/marta">Marta</a> â€” <strong>Liveness</strong>: Adjust personal state sliders and watch AI guidance respond in real time</li>
		</ul>

		<h2>SDK Languages</h2>
		<p>VCP has official SDK implementations in two languages:</p>

		<table>
			<thead>
				<tr>
					<th>Language</th>
					<th>Status</th>
					<th>Best For</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td><strong>Python</strong></td>
					<td>Complete</td>
					<td>Reference implementation, LLM integration, persona logic, API servers</td>
				</tr>
				<tr>
					<td><strong>Rust</strong></td>
					<td>In Progress</td>
					<td>High-performance parsing, WASM/browser, embedded systems, CLI tooling</td>
				</tr>
			</tbody>
		</table>

		<h3>Python</h3>
		<pre><code>{`pip install vcp-python-sdk
from vcp.semantics import encode_csm1, parse_csm1

token = encode_csm1(context)
parsed = parse_csm1(token_string)`}</code></pre>

		<h3>Rust</h3>
		<pre><code>{`// Cargo.toml
[dependencies]
vcp-core = "0.1"

// Usage
use vcp_core::csm1::Csm1Token;
let token = Csm1Token::parse(token_str)?;
let encoded = token.to_string();`}</code></pre>

		<p>
			The Rust crate also compiles to <strong>WebAssembly</strong>, enabling client-side VCP
			token validation directly in the browser via <code>vcp-wasm</code>.
		</p>

		<h2>Next Steps</h2>
		<ul>
			<li><a href="/docs/concepts">Core Concepts</a> â€” Deep dive into VCP architecture</li>
			<li><a href="/docs/csm1-specification">CSM-1 Specification</a> â€” Full token format reference</li>
			<li><a href="/playground">Playground</a> â€” Build tokens interactively</li>
			<li><a href="/demos">All Demos</a> â€” Six interactive demos covering portability, adaptation, liveness, and more</li>
		</ul>
	{/snippet}
</DocsLayout>
